{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term-statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashanth2902/PrashanthPrabhakar/blob/master/token_counts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYCQrU0J5C5"
      },
      "source": [
        "# Empirical Regularities of Language\n",
        "\n",
        "In this first homework assignment, you will familiarize yourself with some empirical regularities of natural language, Shannon entropy and Zipf's Law.\n",
        "\n",
        "Read through this Jupyter notebook and fill in the parts marked with `TODO`. When you're ready to submit, print the notebook as a PDF and upload to Gradescope.\n",
        "\n",
        "## Shannon Entropy\n",
        "\n",
        "Shannon borrowed the concept of entropy from statistical physics to develop _information theory_, focused on encoding and compressing messages. A few years later, in 1950, he applied information theory to analyze human predictive ability—in other words, the entropy of the human language model. You can read the original article, [Prediction and Entropy of Printed English](https://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf), for more details.\n",
        "\n",
        "Your first task is to collect data on how predictable different letters are in an English sentence, depending on how much context in a word or sentence you have.\n",
        "\n",
        "Go to the [Shannon game page](https://www.ccs.neu.edu/home/dasmith/courses/cs6120/shannon/) that we demonstrated in class. We already guessed part of Text 1, so work through Texts 2, 3, and 4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Enter the arrays of numbers of guesses for Texts 2, 3, and 4 here.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "word1 = [17, 1, 13, 1, 1, 1, 1, 1, 26, 1, 1, 1, 1, 2, 1, 1, 1, 1, 24, 1, 19, 4, 1, 1, 3, 1, 1, 1, 3, 2, 10, 1, 6, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "word2 = [1, 3, 1, 7, 4, 1, 3, 1, 11, 4, 8, 10, 7, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 1, 25, 2, 1, 1, 7, 9, 10, 23, 1, 15, 1, 3, 15, 1, 27, 25, 6, 1, 17, 1, 18, 8, 6, 1, 1, 1, 14, 3, 13, 1]\n",
        "word3 = [8, 2, 8, 1, 1, 25, 14, 1, 1, 1, 1, 1, 1, 1, 1, 15, 2, 1, 1, 9, 1, 1, 1, 1, 16, 1, 5, 9, 13, 3, 1, 1, 16, 1, 14, 1, 1, 1, 1, 10, 25, 26, 18, 1, 14, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ],
      "metadata": {
        "id": "n6siYs-p0-_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rearrange the guess data into a two-dimensional array, relating number of characters of context (0, 1, 2, ...) to number of guesses required.\n",
        "\n",
        "In other words, you might look in cell (2, 1) and read \"2\" if the number of times it took one guess to get the right letter with two characters of context was 2."
      ],
      "metadata": {
        "id": "3P2UvFmj2Ep2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create array of counts of guesses. Print out the array so we can see it.\n",
        "all_words = [word1, word2, word3]\n",
        "\n",
        "max_context = max(len(w) for w in all_words)\n",
        "num_guess_options = 28\n",
        "\n",
        "counts = np.zeros((max_context, num_guess_options), dtype=int)\n",
        "\n",
        "for word in all_words:\n",
        "    for context, num_guesses in enumerate(word):\n",
        "        counts[context, num_guesses] += 1\n",
        "\n",
        "print(\"Shape of the counts array:\", counts.shape)\n",
        "print(\"Output Array:\")\n",
        "print(counts)"
      ],
      "metadata": {
        "id": "deK9pKWC1Qp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f6a0d0-a2ca-4044-d15c-eb6246a6c526"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the counts array: (67, 28)\n",
            "Output Array:\n",
            "[[0 1 0 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can compute Shannon's upper and lower bounds on the entropy of your predictive distribution for English. The upper bound, as a function of the number of context characters $N$, is just the Shannon entropy of the distribution of numbers of guesses. In other words, it's the entropy of the original text as &ldquo;reduced&rdquo; by the human encoder to a sequence of numbers of guesses.\n",
        "\n",
        "$F_N = -\\sum_{i=1}^{27} q_i^N \\log_2 q_i^N$\n",
        "\n",
        "where $q_i^N$ is the number of times you took $i$ guesses with $N$ characters of context, i.e., one of the cells in the table you computed. The number of guesses ranges from 1 to 27 since we restrict ourselves to 26 letters plus space. In computing entropy, we define $0 \\log 0 = 0$."
      ],
      "metadata": {
        "id": "BR9BqdYO2eTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the upper bound for each amount of context N and print it out.\n",
        "\n",
        "upper_bounds = []\n",
        "\n",
        "for N in range(counts.shape[0]):\n",
        "    guess_counts_at_N = counts[N]\n",
        "\n",
        "    # Sum these counts to get total number of observations for this context.\n",
        "    total_observations = np.sum(guess_counts_at_N)\n",
        "\n",
        "    if total_observations > 0:\n",
        "        probabilities = guess_counts_at_N / total_observations\n",
        "        non_zero_probs = probabilities[probabilities > 0]\n",
        "        entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
        "        upper_bounds.append(entropy)\n",
        "        # Print the result for each context\n",
        "        print(f\"Context N={N} ({total_observations} observations): F_N = {entropy}\")\n"
      ],
      "metadata": {
        "id": "4ubcFqVv4CjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4576f1b8-c915-448c-afc2-fca6d3a94832"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shannon Entropy Upper Bound (F_N) for each context length N:\n",
            "------------------------------------------------------------\n",
            "Context N=0 (3 observations): F_N = 1.584962500721156\n",
            "Context N=1 (3 observations): F_N = 1.584962500721156\n",
            "Context N=2 (3 observations): F_N = 1.584962500721156\n",
            "Context N=3 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=4 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=5 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=6 (3 observations): F_N = 1.584962500721156\n",
            "Context N=7 (3 observations): F_N = -0.0\n",
            "Context N=8 (3 observations): F_N = 1.584962500721156\n",
            "Context N=9 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=10 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=11 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=12 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=13 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=14 (3 observations): F_N = -0.0\n",
            "Context N=15 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=16 (3 observations): F_N = 1.584962500721156\n",
            "Context N=17 (3 observations): F_N = -0.0\n",
            "Context N=18 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=19 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=20 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=21 (3 observations): F_N = 1.584962500721156\n",
            "Context N=22 (3 observations): F_N = -0.0\n",
            "Context N=23 (3 observations): F_N = -0.0\n",
            "Context N=24 (3 observations): F_N = 1.584962500721156\n",
            "Context N=25 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=26 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=27 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=28 (3 observations): F_N = 1.584962500721156\n",
            "Context N=29 (3 observations): F_N = 1.584962500721156\n",
            "Context N=30 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=31 (3 observations): F_N = -0.0\n",
            "Context N=32 (3 observations): F_N = 1.584962500721156\n",
            "Context N=33 (3 observations): F_N = -0.0\n",
            "Context N=34 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=35 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=36 (3 observations): F_N = -0.0\n",
            "Context N=37 (3 observations): F_N = 1.584962500721156\n",
            "Context N=38 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=39 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=40 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=41 (3 observations): F_N = 1.584962500721156\n",
            "Context N=42 (3 observations): F_N = 1.584962500721156\n",
            "Context N=43 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=44 (3 observations): F_N = 1.584962500721156\n",
            "Context N=45 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=46 (3 observations): F_N = 0.9182958340544896\n",
            "Context N=47 (2 observations): F_N = -0.0\n",
            "Context N=48 (2 observations): F_N = 1.0\n",
            "Context N=49 (2 observations): F_N = 1.0\n",
            "Context N=50 (2 observations): F_N = -0.0\n",
            "Context N=51 (2 observations): F_N = 1.0\n",
            "Context N=52 (2 observations): F_N = 1.0\n",
            "Context N=53 (2 observations): F_N = 1.0\n",
            "Context N=54 (2 observations): F_N = -0.0\n",
            "Context N=55 (2 observations): F_N = 1.0\n",
            "Context N=56 (2 observations): F_N = -0.0\n",
            "Context N=57 (1 observations): F_N = -0.0\n",
            "Context N=58 (1 observations): F_N = -0.0\n",
            "Context N=59 (1 observations): F_N = -0.0\n",
            "Context N=60 (1 observations): F_N = -0.0\n",
            "Context N=61 (1 observations): F_N = -0.0\n",
            "Context N=62 (1 observations): F_N = -0.0\n",
            "Context N=63 (1 observations): F_N = -0.0\n",
            "Context N=64 (1 observations): F_N = -0.0\n",
            "Context N=65 (1 observations): F_N = -0.0\n",
            "Context N=66 (1 observations): F_N = -0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shannon derived a lower bound on entropy from the guess data as\n",
        "\n",
        "$\\sum_{i=1}^{27} i(q_i^N - q_{i+1}^N) \\log_2 i$"
      ],
      "metadata": {
        "id": "NRULmLAM4I5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute thew lower bound for each amount of context N and print it out.\n",
        "\n",
        "i_vals = np.arange(1, 28)\n",
        "log2_i_vals = np.log2(i_vals)\n",
        "\n",
        "lower_bounds = []\n",
        "\n",
        "for N in range(counts.shape[0]):\n",
        "    guess_counts_at_N = counts[N]\n",
        "    total_observations = np.sum(guess_counts_at_N)\n",
        "\n",
        "    if total_observations > 0:\n",
        "        probabilities = guess_counts_at_N / total_observations\n",
        "        q_i = probabilities[1:]\n",
        "        q_i_plus_1 = np.append(probabilities[2:], 0)\n",
        "        diff = q_i - q_i_plus_1\n",
        "        lower_bound = np.sum(i_vals * diff * log2_i_vals)\n",
        "        lower_bounds.append(lower_bound)\n",
        "        print(f\"Context N={N} ({total_observations:d} observations): Lower Bound = {lower_bound}\")"
      ],
      "metadata": {
        "id": "QjRBkxVdB-4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7999f79f-3618-4e06-dfb6-39b27411ef39"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context N=0 (3 observations): Lower Bound = 3.27846128228418\n",
            "Context N=1 (3 observations): Lower Bound = 1.584962500721156\n",
            "Context N=2 (3 observations): Lower Bound = 3.144893957592366\n",
            "Context N=3 (3 observations): Lower Bound = 1.3805698166920966\n",
            "Context N=4 (3 observations): Lower Bound = 1.0817041659455104\n",
            "Context N=5 (3 observations): Lower Bound = 2.0191015756867827\n",
            "Context N=6 (3 observations): Lower Bound = 2.6507133583785776\n",
            "Context N=7 (3 observations): Lower Bound = 0.0\n",
            "Context N=8 (3 observations): Lower Bound = 3.6498315944789823\n",
            "Context N=9 (3 observations): Lower Bound = 1.0817041659455104\n",
            "Context N=10 (3 observations): Lower Bound = 1.4495051818655913\n",
            "Context N=11 (3 observations): Lower Bound = 1.5633186452976027\n",
            "Context N=12 (3 observations): Lower Bound = 1.3805698166920966\n",
            "Context N=13 (3 observations): Lower Bound = 0.6666666666666666\n",
            "Context N=14 (3 observations): Lower Bound = 0.0\n",
            "Context N=15 (3 observations): Lower Bound = 1.7667966751071091\n",
            "Context N=16 (3 observations): Lower Bound = 1.584962500721156\n",
            "Context N=17 (3 observations): Lower Bound = 0.0\n",
            "Context N=18 (3 observations): Lower Bound = 1.999058342665485\n",
            "Context N=19 (3 observations): Lower Bound = 1.5097750043269365\n",
            "Context N=20 (3 observations): Lower Bound = 1.8839909098221668\n",
            "Context N=21 (3 observations): Lower Bound = 1.748370832612177\n",
            "Context N=22 (3 observations): Lower Bound = 0.0\n",
            "Context N=23 (3 observations): Lower Bound = 0.0\n",
            "Context N=24 (3 observations): Lower Bound = 2.7171761893452273\n",
            "Context N=25 (3 observations): Lower Bound = 1.6114889520455513\n",
            "Context N=26 (3 observations): Lower Bound = 1.203213491478937\n",
            "Context N=27 (3 observations): Lower Bound = 1.5097750043269365\n",
            "Context N=28 (3 observations): Lower Bound = 2.613684609781263\n",
            "Context N=29 (3 observations): Lower Bound = 1.584962500721156\n",
            "Context N=30 (3 observations): Lower Bound = 1.5633186452976027\n",
            "Context N=31 (3 observations): Lower Bound = 0.0\n",
            "Context N=32 (3 observations): Lower Bound = 3.0989251985874464\n",
            "Context N=33 (3 observations): Lower Bound = 0.0\n",
            "Context N=34 (3 observations): Lower Bound = 1.732417524324088\n",
            "Context N=35 (3 observations): Lower Bound = 1.999058342665485\n",
            "Context N=36 (3 observations): Lower Bound = 0.0\n",
            "Context N=37 (3 observations): Lower Bound = 2.685768242353447\n",
            "Context N=38 (3 observations): Lower Bound = 1.3333333333333333\n",
            "Context N=39 (3 observations): Lower Bound = 1.5633186452976027\n",
            "Context N=40 (3 observations): Lower Bound = 2.0191015756867827\n",
            "Context N=41 (3 observations): Lower Bound = 3.418912459125522\n",
            "Context N=42 (3 observations): Lower Bound = 3.36703557922889\n",
            "Context N=43 (3 observations): Lower Bound = 1.5633186452976027\n",
            "Context N=44 (3 observations): Lower Bound = 3.710560650754342\n",
            "Context N=45 (3 observations): Lower Bound = 1.203213491478937\n",
            "Context N=46 (3 observations): Lower Bound = 1.7667966751071091\n",
            "Context N=47 (2 observations): Lower Bound = 0.0\n",
            "Context N=48 (2 observations): Lower Bound = 1.3774437510817341\n",
            "Context N=49 (2 observations): Lower Bound = 2.6501950126606637\n",
            "Context N=50 (2 observations): Lower Bound = 0.0\n",
            "Context N=51 (2 observations): Lower Bound = 3.0852649433726285\n",
            "Context N=52 (2 observations): Lower Bound = 3.0286523635301847\n",
            "Context N=53 (2 observations): Lower Bound = 1.950067264945063\n",
            "Context N=54 (2 observations): Lower Bound = 0.0\n",
            "Context N=55 (2 observations): Lower Bound = 2.743434150627884\n",
            "Context N=56 (2 observations): Lower Bound = 0.0\n",
            "Context N=57 (1 observations): Lower Bound = 5.571781724705843\n",
            "Context N=58 (1 observations): Lower Bound = 4.348515545596772\n",
            "Context N=59 (1 observations): Lower Bound = 3.900134529890126\n",
            "Context N=60 (1 observations): Lower Bound = 0.0\n",
            "Context N=61 (1 observations): Lower Bound = 0.0\n",
            "Context N=62 (1 observations): Lower Bound = 0.0\n",
            "Context N=63 (1 observations): Lower Bound = 5.1972525729722605\n",
            "Context N=64 (1 observations): Lower Bound = 2.7548875021634682\n",
            "Context N=65 (1 observations): Lower Bound = 5.086166327180322\n",
            "Context N=66 (1 observations): Lower Bound = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Now let's look at some text data directly to see the skewed distribution of tokens predicted by Zipf's Law. Recall that Zipf's law states that a word's rank (from the most common word at rank 1 on down) to its frequency is approximately a constant, i.e., $r \\cdot f = k$. Equivalently, we can divide both sides by the total number of tokens $N$ to get $r \\cdot P_r = c$, where $c = k/N$ and $P_r = f/N$ is the _relative frequency_ of word $r$.\n",
        "\n",
        "We start by downloading a sample of 1000 open-access English books from [Project Gutenberg](https://gutenberg.org/)."
      ],
      "metadata": {
        "id": "QRingmXrfJSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If your local environment doesn't have the wget command,\n",
        "# you can comment this out and download it manually.\n",
        "!wget \"http://khoury.northeastern.edu/home/dasmith/pg-sample.json.gz\""
      ],
      "metadata": {
        "id": "iwJS1ClihN_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47982f38-4b2f-4173-958d-1bb826e6ad05"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-18 23:50:39--  http://khoury.northeastern.edu/home/dasmith/pg-sample.json.gz\n",
            "Resolving khoury.northeastern.edu (khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to khoury.northeastern.edu (khoury.northeastern.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150984197 (144M) [application/x-gzip]\n",
            "Saving to: ‘pg-sample.json.gz.1’\n",
            "\n",
            "pg-sample.json.gz.1 100%[===================>] 143.99M  96.6MB/s    in 1.5s    \n",
            "\n",
            "2025-09-18 23:50:41 (96.6 MB/s) - ‘pg-sample.json.gz.1’ saved [150984197/150984197]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file is compressed with gzip and is in a JSON lines format. Each line is one JSON record, which we parse with the `json` library.\n",
        "\n",
        "Here we print out the keys in the first record: `id`, `author`, `title`, and `text`."
      ],
      "metadata": {
        "id": "rcOHU_GPiYwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip, json\n",
        "for line in gzip.open(\"pg-sample.json.gz\", mode=\"rt\", encoding=\"utf-8\"):\n",
        "  rec = json.loads(line)\n",
        "  print(rec.keys())\n",
        "  print(rec['author'])\n",
        "  print(rec['title'])\n",
        "  print(rec['text'][0:100])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L3x1DRZhfTb",
        "outputId": "d4c84f09-86e8-4187-eb3c-c177228634e8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'author', 'title', 'text'])\n",
            "Jefferson, Thomas\n",
            "The Declaration of Independence of the United States of America\n",
            "\n",
            "\n",
            "This is a retranscription of one of the first Project\n",
            "Gutenberg Etexts, offically dated December 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task now is to **tokenize** the text in the `text` field of each record into an array of words. Later on in this course, we will discuss learning better tokenizers. For now, you should separate words on whitespace (space, newline, tab) and punctuation. Convert the tokens to lower case, and keep only those tokens that have at least one letter a-z in them. In general, numerals in text tend not to follow Zipf's law but [Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law).\n",
        "\n",
        "You might use _regular expressions_ (e.g., the `re.split` function) to help with tokenization and filtering.\n",
        "\n",
        "After you have tokenized, compute $N$, the total number of tokens in the corpus and print it out."
      ],
      "metadata": {
        "id": "XbfYV9Fyi2UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an array of tokens in the corpus\n",
        "# Compute the total number of tokens N and print it out.\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Define the path to your smaller, user-provided file\n",
        "file_path = \"pg-sample.json.gz\"\n",
        "all_tokens = []\n",
        "\n",
        "try:\n",
        "    # Open the file in read-only text mode with UTF-8 encoding\n",
        "    with open(file_path, mode=\"rt\", encoding=\"utf-8\") as f:\n",
        "        # Load the entire JSON array from the file\n",
        "        records = json.load(f)\n",
        "\n",
        "        # Iterate through each book record in the list\n",
        "        for record in records:\n",
        "            text = record.get('text', '')\n",
        "\n",
        "            # Split the text on any sequence of one or more non-alphabetic characters\n",
        "            tokens = re.split(r'[^a-zA-Z]+', text)\n",
        "\n",
        "            # Create a new list of tokens that are lowercased and not empty\n",
        "            processed_tokens = [token.lower() for token in tokens if token]\n",
        "\n",
        "            # Add the tokens from this book to our master list\n",
        "            all_tokens.extend(processed_tokens)\n",
        "\n",
        "    # Compute the total number of tokens (N) by getting the length of the list\n",
        "    N = len(all_tokens)\n",
        "\n",
        "    # Print the final count\n",
        "    print(\"Tokenization complete.\")\n",
        "    print(f\"Total number of tokens (N): {N:,}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found in this directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "EsVzek9vkgFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c7263f-ed02-4644-9afa-cb127ad1aa01"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, count the frequency each unigram (distinct word) in the corpus and sort them in an array in descending order of frequency. The first item in your array should be the most common word. Print out that word and its frequency"
      ],
      "metadata": {
        "id": "XkRccWKsk4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an arrary of unigrams in descending order of frequency.\n",
        "# Print the most common word and its frequency."
      ],
      "metadata": {
        "id": "jhJ7ec3jk2eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can look at the Zipf's law relationship between rank and relative frequency (i.e., frequency divided by $N$). Plot the data using a python graphing package such as matplotlib, plotly, or plotnine. This doesn't have to be a fancy graph, so use whatever you're familiar with. Both axes should be on a log scale. If your package doesn't support log scales, you can take the log of the rank and relative frequency yourself before plotting. Recall that since python arrays are zero-indexed, the rank 1 word will be element 0 of your sorted array."
      ],
      "metadata": {
        "id": "ubHNre1ollvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot rank vs. relative frequency of unigrams."
      ],
      "metadata": {
        "id": "H1wq5OGBmJP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take your array of tokens and compute the counts of both the bigrams and trigrams and sort them in descending order of frequency. Print out the most common bigram and trigram."
      ],
      "metadata": {
        "id": "dTVQFNXOmc34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute sorted bigram and trigram statistics.\n",
        "# Print out the most common bigram and trigram.\n",
        "# Plot rank vs. relative frequency for bigrams and trigrams.\n",
        "# You may make separate plots or put them on the same plot and label them."
      ],
      "metadata": {
        "id": "glYoeIsJmm_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Finally, write your visual impressions of the fit of the unigram, bigram, and trigram distributions. This doesn't need to be statistically rigorous."
      ],
      "metadata": {
        "id": "TdUTKyE6m0qF"
      }
    }
  ]
}